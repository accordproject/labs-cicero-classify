{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T23:47:31.941877Z",
     "start_time": "2021-08-19T23:47:31.636509Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import numpy as np\n",
    "\n",
    "from ..core.config import MONGODB_URL,DATABASE_NAME, NER_LABEL_COLLECTION, LABEL_COLLECTION, LABEL_RETRAIN_QUEUE_COLLECTION\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T01:53:28.806293Z",
     "start_time": "2021-08-19T01:53:28.793720Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def get_training_dataframe(train_data_search_filter = {}):\n",
    "    client = pymongo.MongoClient(MONGODB_URL)\n",
    "    col = client[DATABASE_NAME][NER_LABEL_COLLECTION]\n",
    "\n",
    "    result = col.find(train_data_search_filter)\n",
    "    print(\"Reading Data from MongoDB...\")\n",
    "    result = list(result)\n",
    "    print(\"Processing Data...\")\n",
    "    df = pd.DataFrame()\n",
    "    for i, sentence in enumerate(tqdm(result)):\n",
    "        sentense_df = pd.DataFrame(columns=[\"Sentence #\", \"text\", \"labels\"], data = sentence[\"text_and_labels\"])\n",
    "        sentense_df[\"Sentence #\"] = str(sentence[\"_id\"])\n",
    "        df = df.append(sentense_df)\n",
    "    return df\n",
    "def get_training_data_by_df_according_to_label_name(df, label_name):\n",
    "    label_col = client[DATABASE_NAME][LABEL_COLLECTION]\n",
    "\n",
    "    label_info = label_col.find_one({\"label_name\": label_name})\n",
    "    alias_labels = label_col.find({\"alias_as\": {\"$in\": [label_name]}})\n",
    "\n",
    "    alias = []\n",
    "    for alias_label in alias_labels:\n",
    "        alias.append(alias_label[\"label_name\"])\n",
    "    alias = alias + label_info[\"inherit\"]\n",
    "\n",
    "    wanted_label = [label_name] + alias\n",
    "\n",
    "    def label_data(label):\n",
    "        if set(label).intersection(set(wanted_label)):\n",
    "            return label_name\n",
    "        else:\n",
    "            return \"O\"\n",
    "\n",
    "    df[label_name] = list(df[\"labels\"].apply(label_data))\n",
    "\n",
    "    sentences = df.groupby(\"Sentence #\")[\"text\"].apply(list).values\n",
    "    tags = df.groupby(\"Sentence #\")[label_name].apply(list).values\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T01:50:10.996091Z",
     "start_time": "2021-08-19T01:50:10.069084Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T01:50:17.022375Z",
     "start_time": "2021-08-19T01:50:11.575919Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T01:50:17.621104Z",
     "start_time": "2021-08-19T01:50:17.605678Z"
    }
   },
   "outputs": [],
   "source": [
    "class NER_Dataset_for_Adapter(Dataset):\n",
    "    def __init__(self, tokenizer, df, label_name):\n",
    "        self.label_name = label_name\n",
    "        self.mode = \"train\"\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.sentences, self.tags = get_training_data_by_df_according_to_label_name(df, label_name)\n",
    "        self.len = len(self.sentences)\n",
    "\n",
    "\n",
    "        labels = [\"O\", label_name]\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "            labels = [\"O\", label_name]\n",
    "            self.label_map = {}\n",
    "            for i, label in enumerate(labels):\n",
    "                self.label_map[label] = i\n",
    "\n",
    "            possible_labels = np.array(range(len(labels))).reshape(-1, 1)\n",
    "            self.oneHotEncoder = OneHotEncoder()\n",
    "            self.oneHotEncoder.fit(possible_labels)\n",
    "        else:\n",
    "            self.label_map = None\n",
    "\n",
    "        self.tokenizer = tokenizer  # RoBERTa tokenizer\n",
    "        self.O_label = self.label_map[\"O\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            label = [\"O\"] + self.tags[idx] + [\"O\"]\n",
    "\n",
    "            label = np.array(label)\n",
    "            label = label.reshape(-1,1)\n",
    "\n",
    "            label = np.apply_along_axis(self.split_one_hot_multiTags, 1, label)\n",
    "            label_tensor = torch.tensor(label, dtype = torch.float32)\n",
    "\n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [tokenizer.cls_token]\n",
    "        word_pieces += self.sentences[idx]\n",
    "        word_pieces += [tokenizer.sep_token]\n",
    "\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0\n",
    "        segments_tensor = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def split_one_hot_multiTags(self, tags):\n",
    "        # tags = ['B-org|Party|String']\n",
    "        tags = tags[0]\n",
    "        tags = tags.split(\"|\")\n",
    "\n",
    "\n",
    "        tags_num = list(map(lambda x: self.label_map[x], tags))\n",
    "        #[5, 20, 23]\n",
    "\n",
    "        tags_num = np.array(tags_num).reshape(-1,1)\n",
    "\n",
    "        tags_one_hot = self.oneHotEncoder.transform(tags_num).toarray()\n",
    "\n",
    "        tags_one_hot = tags_one_hot.sum(axis = 0)\n",
    "\n",
    "        #return torch.tensor(tags_one_hot, dtype = torch.float32)\n",
    "\n",
    "        return tags_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:18.352186Z",
     "start_time": "2021-08-19T08:27:18.348060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Env Virable (Might From Config)\n",
    "BATCH_SIZE = 16\n",
    "default_filter = {}\n",
    "device_id = 0\n",
    "device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "Adapter_Patch = \".\"\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "# When Each Train\n",
    "# Run When Set Up\n",
    "if os.path.isdir(f\"{Adapter_Patch}/save_adapters\") == False:\n",
    "    os.mkdir(f\"{Adapter_Patch}/save_adapters\")\n",
    "if os.path.isdir(f\"{Adapter_Patch}/save_heads\") == False:\n",
    "    os.mkdir(f\"{Adapter_Patch}/save_heads\")\n",
    "\n",
    "dateStamp = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# On Each Iter\n",
    "train_data_search_filter = {}\n",
    "label_name = \"Party87\"\n",
    "Epoch_Times = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:20:29.282734Z",
     "start_time": "2021-08-19T03:20:29.279515Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T03:20:29.491408Z",
     "start_time": "2021-08-19T03:20:29.488429Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:25.227300Z",
     "start_time": "2021-08-19T08:27:20.319867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data from MongoDB...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13485/3073223546.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_13485/3526618783.py\u001b[0m in \u001b[0;36mget_training_dataframe\u001b[0;34m(train_data_search_filter)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_search_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading Data from MongoDB...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__manipulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                 \u001b[0m_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__sock_mgr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                                     self.__exhaust)\n\u001b[0;32m-> 1175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m__send_message\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             response = client._run_operation(\n\u001b[0;32m-> 1045\u001b[0;31m                 operation, self._unpack_response, address=self.__address)\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOperationFailure\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_CURSOR_CLOSED_ERRORS\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exhaust\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_run_operation\u001b[0;34m(self, operation, unpack_res, address)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         return self._retryable_read(\n\u001b[1;32m   1425\u001b[0m             \u001b[0m_cmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             address=address, retryable=isinstance(operation, message._Query))\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_retry_with_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretryable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbulk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_retryable_read\u001b[0;34m(self, func, read_pref, session, address, retryable)\u001b[0m\n\u001b[1;32m   1523\u001b[0m                         \u001b[0;31m# not support retryable reads, raise the last error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mlast_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mServerSelectionTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mretrying\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_cmd\u001b[0;34m(session, server, sock_info, secondary_ok)\u001b[0m\n\u001b[1;32m   1420\u001b[0m             return server.run_operation(\n\u001b[1;32m   1421\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_ok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_listeners\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m                 unpack_res)\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m         return self._retryable_read(\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/server.py\u001b[0m in \u001b[0;36mrun_operation\u001b[0;34m(self, sock_info, operation, set_secondary_okay, listeners, unpack_res)\u001b[0m\n\u001b[1;32m    124\u001b[0m                               \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodec_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                               \u001b[0mlegacy_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlegacy_response\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                               user_fields=user_fields)\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cmd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m_unpack_response\u001b[0;34m(self, response, cursor_id, codec_options, user_fields, legacy_response)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                          user_fields=None, legacy_response=False):\n\u001b[1;32m   1109\u001b[0m         return response.unpack_response(cursor_id, codec_options, user_fields,\n\u001b[0;32m-> 1110\u001b[0;31m                                         legacy_response)\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_preference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/pymongo/message.py\u001b[0m in \u001b[0;36munpack_response\u001b[0;34m(self, cursor_id, codec_options, user_fields, legacy_response)\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlegacy_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m         return bson._decode_all_selective(\n\u001b[0;32m-> 1671\u001b[0;31m             self.payload_document, codec_options, user_fields)\n\u001b[0m\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcommand_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/bson/__init__.py\u001b[0m in \u001b[0;36m_decode_all_selective\u001b[0;34m(data, codec_options, fields)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     \"\"\"\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcodec_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/adapter/lib/python3.7/site-packages/bson/objectid.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, oid)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0m_type_marker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \"\"\"Initialize a new ObjectId.\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "df = get_training_dataframe(default_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:41.486472Z",
     "start_time": "2021-08-19T08:27:41.480954Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "client = pymongo.MongoClient(MONGODB_URL)\n",
    "col = client[DATABASE_NAME][NER_LABEL_COLLECTION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:42.355001Z",
     "start_time": "2021-08-19T08:27:42.091763Z"
    }
   },
   "outputs": [],
   "source": [
    "wanted_id = list(map(lambda x: str(x[\"_id\"]),\n",
    "                     list(col.find(train_data_search_filter, {\"_id\": True}))))\n",
    "\n",
    "target_df = df[df[\"Sentence #\"].isin(wanted_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:45.007560Z",
     "start_time": "2021-08-19T08:27:42.808495Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset = NER_Dataset_for_Adapter(tokenizer, target_df, label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:46.154523Z",
     "start_time": "2021-08-19T08:27:46.151886Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:46.545297Z",
     "start_time": "2021-08-19T08:27:46.540627Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "\n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = [s[2] for s in samples]\n",
    "        label_ids = pad_sequence(label_ids, \n",
    "                                  batch_first=True)\n",
    "    else:\n",
    "        label_ids = None\n",
    "\n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "\n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "\n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:47.150994Z",
     "start_time": "2021-08-19T08:27:47.126169Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:27:48.278138Z",
     "start_time": "2021-08-19T08:27:47.365868Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModelWithHeads\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T11:21:30.127402Z",
     "start_time": "2021-08-19T11:21:30.119502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                [O]\n",
       "1                [O]\n",
       "2                [O]\n",
       "3                [O]\n",
       "4                [O]\n",
       "          ...       \n",
       "2                [O]\n",
       "3    [Party, String]\n",
       "4                [O]\n",
       "5     [TemporalUnit]\n",
       "6                [O]\n",
       "Name: labels, Length: 1048582, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T08:39:40.730125Z",
     "start_time": "2021-08-19T08:27:49.180862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Party87: epoch 0\n",
      "\tLoss: 0.6863803863525391\n",
      "\tLoss: 0.26608678698539734\n",
      "\tLoss: 0.1739221066236496\n",
      "\tLoss: 0.12224006652832031\n",
      "\tLoss: 0.13584811985492706\n",
      "\tLoss: 0.14804978668689728\n",
      "\tLoss: 0.07767172157764435\n",
      "\tLoss: 0.1210523322224617\n",
      "\tLoss: 0.10067823529243469\n",
      "\tLoss: 0.07805242389440536\n",
      "\tLoss: 0.08767428249120712\n",
      "\tLoss: 0.14710210263729095\n",
      "\tLoss: 0.09772222489118576\n",
      "\tLoss: 0.04824133589863777\n",
      "\tLoss: 0.10125719010829926\n",
      "\tLoss: 0.09478776901960373\n",
      "\tLoss: 0.16918572783470154\n",
      "\tLoss: 0.06986373662948608\n",
      "\tLoss: 0.10380092263221741\n",
      "\tLoss: 0.12744483351707458\n",
      "\tLoss: 0.07507335394620895\n",
      "\tLoss: 0.11341594904661179\n",
      "\tLoss: 0.06651750952005386\n",
      "\tLoss: 0.09243737161159515\n",
      "\tLoss: 0.06899833679199219\n",
      "\tLoss: 0.1170530915260315\n",
      "\tLoss: 0.05675486847758293\n",
      "\tLoss: 0.07523295283317566\n",
      "\tLoss: 0.08001790195703506\n",
      "\tLoss: 0.08097665756940842\n",
      "\n",
      "Party87: epoch 1\n",
      "\tLoss: 0.1378442794084549\n",
      "\tLoss: 0.11058449000120163\n",
      "\tLoss: 0.10486708581447601\n",
      "\tLoss: 0.06578885018825531\n",
      "\tLoss: 0.060900747776031494\n",
      "\tLoss: 0.10366994142532349\n",
      "\tLoss: 0.03767190873622894\n",
      "\tLoss: 0.0864952951669693\n",
      "\tLoss: 0.06603756546974182\n",
      "\tLoss: 0.05118601396679878\n",
      "\tLoss: 0.06581883877515793\n",
      "\tLoss: 0.10495166480541229\n",
      "\tLoss: 0.09265226125717163\n",
      "\tLoss: 0.044894471764564514\n",
      "\tLoss: 0.07681594043970108\n",
      "\tLoss: 0.07009340077638626\n",
      "\tLoss: 0.14115390181541443\n",
      "\tLoss: 0.06232602521777153\n",
      "\tLoss: 0.09577704966068268\n",
      "\tLoss: 0.1160254180431366\n",
      "\tLoss: 0.06997179985046387\n",
      "\tLoss: 0.10687059909105301\n",
      "\tLoss: 0.0521627813577652\n",
      "\tLoss: 0.06612824648618698\n",
      "\tLoss: 0.0537639744579792\n",
      "\tLoss: 0.09591279178857803\n",
      "\tLoss: 0.047627802938222885\n",
      "\tLoss: 0.06709334254264832\n",
      "\tLoss: 0.08482469618320465\n",
      "\tLoss: 0.08574210107326508\n"
     ]
    }
   ],
   "source": [
    "model = RobertaModelWithHeads.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    "    )\n",
    "\n",
    "try:\n",
    "    model.add_adapter(label_name)\n",
    "    model.add_tagging_head(\n",
    "        label_name,\n",
    "        num_labels=1\n",
    "      )\n",
    "except: pass\n",
    "model.train_adapter(label_name)\n",
    "model = model.to(device)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": 1e-5,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]\n",
    "optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters, lr=1e-4)\n",
    "\n",
    "for epoch in range(Epoch_Times):\n",
    "    print(f\"\\n{label_name}: epoch {epoch}\")\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        outputs = model(input_ids = tokens_tensors,\n",
    "            attention_mask=masks_tensors,\n",
    "            token_type_ids=segments_tensors)\n",
    "\n",
    "\n",
    "        logits = outputs[0]\n",
    "\n",
    "        current_label = labels.view(-1, labels.shape[-1])[:, trainset.label_map[label_name]]\n",
    "        current_label = current_label.view(-1)\n",
    "\n",
    "        active_logits = logits.view(-1, logits.shape[-1])[masks_tensors.view(-1) == 1]\n",
    "        active_labels = current_label[masks_tensors.view(-1)== 1]\n",
    "\n",
    "        actual = current_label[masks_tensors.view(-1)== 1].float().view(-1,1)\n",
    "\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        loss = loss_fct(active_logits, actual)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"\\tLoss: {loss}\")\n",
    "    \"\"\"filename = f\"{label_name}_epoch_{epoch}_{dateStamp}\"\n",
    "    model.save_adapter(f\"{Adapter_Patch}/save_adapters/{filename}\", model.active_adapters[0])\n",
    "    model.save_head(f\"{Adapter_Patch}/save_heads/{filename}\", model.active_head)\"\"\"\n",
    "filename = f\"{label_name}_epoch_{Epoch_Times}_{dateStamp}\"\n",
    "model.save_adapter(f\"{Adapter_Patch}/save_adapters/{filename}\", model.active_adapters[0])\n",
    "model.save_head(f\"{Adapter_Patch}/save_heads/{filename}\", model.active_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
