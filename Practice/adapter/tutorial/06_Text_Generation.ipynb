{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe_W__U6uFBs"
   },
   "source": [
    "First we need to download the dataset. In this case we use a datasets containing poems. By doing so we train the model to create its own poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBuzA0h3n4-z",
    "outputId": "aded08bc-6647-43d9-c2e3-0bdd831e3a33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (/home/eason/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 105\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"poem_sentiment\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neZt29NKuT11"
   },
   "source": [
    "Before training we need to preprocess the dataset. We tokenize the entries in the dataset and remove all columns we don't need to train the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3Lz6h9Zo9t0",
    "outputId": "9fdb9f57-6aaf-4d09-aa78-07e4049029f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09dbbeaa8873457e8adf9f52aef5c233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a58a628e16e4c3ab02664e3d33801e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6815eb86b244e38a098d264367eaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  encoding = tokenizer(batch[\"verse_text\"])\n",
    "  # For language modeling the labels need to be the input_ids\n",
    "  #encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "  return encoding\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# The GPT-2 tokenizer does not have a padding token. In order to process the data \n",
    "# in batches we set one here \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "column_names = dataset[\"train\"].column_names\n",
    "dataset = dataset.map(encode_batch, remove_columns=column_names, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqJWsH5_ENkb"
   },
   "source": [
    "Next we concatenate the documents in the dataset and create chunks with a length of `block_size`. This is beneficial for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ff5lRmnA6R7",
    "outputId": "afc8b3c0-d70d-4360-f24f-6bffba720957"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e697e99a3d40b4b0b05d85f11584c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768c453de7f7424aa03c888245fdad2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8508dce35d8e4f61b3fa4288c3e48054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 50\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "  # Concatenate all texts.\n",
    "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "  # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "  # customize this part to your needs.\n",
    "  total_length = (total_length // block_size) * block_size\n",
    "  # Split by chunks of max_len.\n",
    "  result = {\n",
    "    k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "  }\n",
    "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "  return result\n",
    "\n",
    "dataset = dataset.map(group_texts,batched=True,)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT56RfOGFXvt"
   },
   "source": [
    "Next we create the model and add our new adapter.Let's just call it `poem` since it is trained to create new poems. Then we activate it and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ioLpFbOfnPE6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# add new adapter\n",
    "model.add_adapter(\"poem\")\n",
    "# activate adapter for training\n",
    "model.train_adapter(\"poem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkCUz8B6Fw5E"
   },
   "source": [
    "The last thing we need to do before we can start training is create the trainer. As trainingsargumnÃ©nts we choose a learningrate of 1e-4. Feel free to play around with the paraeters and see how they affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "stDwnIEApmNu"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./examples\", \n",
    "  do_train=True,\n",
    "  remove_unused_columns=False,\n",
    "  learning_rate=5e-4,\n",
    "  num_train_epochs=3,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "Mtjczmcxqv-l",
    "outputId": "1c386f87-f5cd-4791-d4de-4fd8efe74583"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=33, training_loss=5.555782896099669, metrics={'train_runtime': 5.5207, 'train_samples_per_second': 5.977, 'total_flos': 19852958822400.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pkNcsRzGCQo"
   },
   "source": [
    "Now that we have a trained udapter we save it for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_Q_pKmUkqx7P"
   },
   "outputs": [],
   "source": [
    "model.save_adapter(\"adapter_poem\", \"poem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjcY1WnaGIxi"
   },
   "source": [
    "With our trained adapter we want to create some poems. In order to do this we create a GPT2LMHeadModel wich is best suited for language generation. Then we load our trained adapter. Finally we have to choose the start of our poem. If you want your poem to start differently just change `PREFIX` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "m4TML7t2mZRp"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# You can also load your locally trained adapter\n",
    "model.load_adapter(\"adapter_poem\")\n",
    "model.set_active_adapters(\"poem\")\n",
    "\n",
    "PREFIX = \"In the night\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0tFdm1RGtma"
   },
   "source": [
    "For the generation we need to tokenize the prefix first and then pass it to the model. In this case we create five possible continuations for the beginning we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"In the night\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6iiEOwfsdVd",
    "outputId": "eac599d2-2ddf-4d49-e550-6ad7ad14c411"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(PREFIX, return_tensors=\"pt\")\n",
    "output_sequence = model.generate(\n",
    "  input_ids=encoding[\"input_ids\"],\n",
    "  attention_mask=encoding[\"attention_mask\"],\n",
    "  do_sample=True,\n",
    "  num_return_sequences=5,\n",
    "  max_length = 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ4uCZaZG68k"
   },
   "source": [
    "Lastly we want to see what the model actually created. Too de this we need to decode the tokens from ids back to words and remove the end of sentence tokens. You can easily use this code with an other dataset. Don't forget to share your adapters at [AdapterHub](https://adapterhub.ml/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TX3QpuWPtrol",
    "outputId": "e5260e2e-7681-49a7-d05e-6f4c4f7aeeef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SEQUENCE 1 ===\n",
      "In the night, on Monday, and just the way of things went over in the home game.\n",
      "\n",
      "The game came down to a narrow majority vote.\n",
      "\n",
      "They finished third and then just after. Just so.\n",
      "\n",
      "\n",
      "Just \n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "In the night out, Mr. Aoki was taken to the Todomac in North, where he died, with the help of his wife, with the consequence that that he died on the 4th November.\n",
      "\n",
      "The following day i\n",
      "=== GENERATED SEQUENCE 3 ===\n",
      "In the night of December 28, 1968 in Berkeley, Berkeley with the intent to take down a long-haired man who had turned out to be Hitler. All around Berkeley, a large group of student workers, many of which had been in the mai\n",
      "=== GENERATED SEQUENCE 4 ===\n",
      "In the night, he is in an \"out of heart\".\n",
      "\n",
      "\"It is a surprise because he's one. He got a little boy. He gets his little boy boy,\" Mr Aamuja told the BBC Channel 1.\n",
      "=== GENERATED SEQUENCE 5 ===\n",
      "In the night heat, on cold roads he was an actor and a gun. \"We played him and a script that the film will have it.\" He says it was \"too bad\".\n",
      "\"It's not a bad. We were a lo\n"
     ]
    }
   ],
   "source": [
    " for generated_sequence_idx, generated_sequence in enumerate(output_sequence):\n",
    "        print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        # Remove EndOfSentence Tokens\n",
    "        text = text[: text.find(tokenizer.eos_token)]\n",
    "\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): GPT2AttentionAdaptersModule(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): GPT2OutputAdaptersModule(\n",
       "          (adapters): ModuleDict(\n",
       "            (poem): Adapter(\n",
       "              (non_linearity): Activation_Function_Class()\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class()\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_Text_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
