{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:37:54.345948Z",
     "start_time": "2021-08-17T02:37:41.706048Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-17 10:37:46.661753: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:38:05.321531Z",
     "start_time": "2021-08-17T02:37:54.347359Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:38:05.325684Z",
     "start_time": "2021-08-17T02:38:05.322886Z"
    }
   },
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "answer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:38:05.332881Z",
     "start_time": "2021-08-17T02:38:05.327508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 70 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:38:05.344338Z",
     "start_time": "2021-08-17T02:38:05.334172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "how           2,129\n",
      "many          2,116\n",
      "parameters   11,709\n",
      "does          2,515\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "have          2,031\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "is            2,003\n",
      "really        2,428\n",
      "big           2,502\n",
      ".             1,012\n",
      ".             1,012\n",
      ".             1,012\n",
      "it            2,009\n",
      "has           2,038\n",
      "24            2,484\n",
      "-             1,011\n",
      "layers        9,014\n",
      "and           1,998\n",
      "an            2,019\n",
      "em            7,861\n",
      "##bed         8,270\n",
      "##ding        4,667\n",
      "size          2,946\n",
      "of            1,997\n",
      "1             1,015\n",
      ",             1,010\n",
      "02            6,185\n",
      "##4           2,549\n",
      ",             1,010\n",
      "for           2,005\n",
      "a             1,037\n",
      "total         2,561\n",
      "of            1,997\n",
      "340          16,029\n",
      "##m           2,213\n",
      "parameters   11,709\n",
      "!               999\n",
      "altogether   10,462\n",
      "it            2,009\n",
      "is            2,003\n",
      "1             1,015\n",
      ".             1,012\n",
      "34            4,090\n",
      "##gb         18,259\n",
      ",             1,010\n",
      "so            2,061\n",
      "expect        5,987\n",
      "it            2,009\n",
      "to            2,000\n",
      "take          2,202\n",
      "a             1,037\n",
      "couple        3,232\n",
      "minutes       2,781\n",
      "to            2,000\n",
      "download      8,816\n",
      "to            2,000\n",
      "your          2,115\n",
      "cola         15,270\n",
      "##b           2,497\n",
      "instance      6,013\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
    "# tokenizer's behavior, let's also get the token strings and display them.\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "# For each token and its id...\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    \n",
    "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "    \n",
    "    # Print the token string and its ID in two columns.\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:38:05.348095Z",
     "start_time": "2021-08-17T02:38:05.345414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Search the input_ids for the first instance of the `[SEP]` token.\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "# The number of segment A tokens includes the [SEP] token istelf.\n",
    "num_seg_a = sep_index + 1\n",
    "\n",
    "# The remainder are segment B.\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "# Construct the list of 0s and 1s.\n",
    "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "# There should be a segment_id for every input token.\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:38:05.351207Z",
     "start_time": "2021-08-17T02:38:05.349129Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:39:33.103905Z",
     "start_time": "2021-08-17T02:39:32.797301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run our example through the model.\n",
    "start_scores, end_scores = model(torch.tensor([input_ids]),token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:39:47.477221Z",
     "start_time": "2021-08-17T02:39:47.197392Z"
    }
   },
   "outputs": [],
   "source": [
    "result = model(torch.tensor([input_ids]),token_type_ids=torch.tensor([segment_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:39:51.584083Z",
     "start_time": "2021-08-17T02:39:51.578013Z"
    }
   },
   "outputs": [],
   "source": [
    "a, b = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:42:21.070457Z",
     "start_time": "2021-08-17T02:42:21.059532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.argmax(result[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:42:24.546160Z",
     "start_time": "2021-08-17T02:42:24.534682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(result[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(result[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T02:42:28.323073Z",
     "start_time": "2021-08-17T02:42:28.310700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340 ##m\"\n"
     ]
    }
   ],
   "source": [
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "answer_start = torch.argmax(result[a])\n",
    "answer_end = torch.argmax(result[b])\n",
    "\n",
    "# Combine the tokens in the answer and print it out.\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "\n",
    "print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
