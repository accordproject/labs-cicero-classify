{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a9e003",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:02:16.295846Z",
     "start_time": "2021-07-31T23:02:15.462061Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data_path = \"./NER_multilabel_data_v2.csv\"\n",
    "BATCH_SIZE = 16\n",
    "def get_trainset_data_loader(tokenizer, data_path = data_path,\n",
    "                             BATCH_SIZE = BATCH_SIZE):\n",
    "    df = pd.read_csv(data_path)\n",
    "\n",
    "    all_tags = df.newTag\n",
    "\n",
    "    all_tags = set(all_tags)\n",
    "\n",
    "    all_tags = \"|\".join(all_tags)\n",
    "    all_tags = all_tags.split(\"|\")\n",
    "    all_tags = set(all_tags)\n",
    "    all_tags = list(all_tags)\n",
    "\n",
    "\n",
    "    def process_csv(data_path):\n",
    "        df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "        df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "        sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "        tags = df.groupby(\"Sentence #\")[\"newTag\"].apply(list).values\n",
    "        return sentences, tags\n",
    "\n",
    "    sentences, tags = process_csv(data_path)\n",
    "\n",
    "    from torch.utils.data import Dataset\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class NER_Dataset(Dataset):\n",
    "        # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "        def __init__(self, mode, tokenizer, data_path, labels):\n",
    "            assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "            self.mode = mode\n",
    "            # 大數據你會需要用 iterator=True\n",
    "            self.sentences, self.tags = process_csv(data_path)\n",
    "            self.len = len(self.sentences)\n",
    "\n",
    "\n",
    "            if mode != \"test\":\n",
    "                self.label_map = {}\n",
    "                for i in range(len(labels)):\n",
    "                    self.label_map[labels[i]] = i\n",
    "\n",
    "                possible_labels = np.array(range(len(labels))).reshape(-1, 1)\n",
    "                self.oneHotEncoder = OneHotEncoder()\n",
    "                self.oneHotEncoder.fit(possible_labels)\n",
    "            else:\n",
    "                self.label_map = None\n",
    "\n",
    "            self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "            self.O_label = self.label_map[\"O\"]\n",
    "\n",
    "\n",
    "        # 定義回傳一筆訓練 / 測試數據的函式\n",
    "        def __getitem__(self, idx):\n",
    "            if self.mode == \"test\":\n",
    "                label_tensor = None\n",
    "            else:\n",
    "                label = [\"O\"] + self.tags[idx] + [\"O\"]\n",
    "\n",
    "                label = np.array(label)\n",
    "                label = label.reshape(-1,1)\n",
    "\n",
    "                label = np.apply_along_axis(self.split_one_hot_multiTags, 1, label)\n",
    "                label_tensor = torch.tensor(label, dtype = torch.float32)\n",
    "\n",
    "            # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "            word_pieces = ['[CLS]']\n",
    "            word_pieces += self.sentences[idx]\n",
    "            word_pieces += ['[SEP]']\n",
    "\n",
    "            ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "            tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "            # 將第一句包含 [SEP] 的 token 位置設為 0\n",
    "            segments_tensor = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "            return (tokens_tensor, segments_tensor, label_tensor)\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.len\n",
    "\n",
    "        def split_one_hot_multiTags(self, tags):\n",
    "            # tags = ['B-org|Party|String']\n",
    "            tags = tags[0]\n",
    "            tags = tags.split(\"|\")\n",
    "\n",
    "\n",
    "            tags_num = list(map(lambda x: self.label_map[x], tags))\n",
    "            #[5, 20, 23]\n",
    "\n",
    "            tags_num = np.array(tags_num).reshape(-1,1)\n",
    "\n",
    "            tags_one_hot = self.oneHotEncoder.transform(tags_num).toarray()\n",
    "\n",
    "            tags_one_hot = tags_one_hot.sum(axis = 0)\n",
    "\n",
    "            #return torch.tensor(tags_one_hot, dtype = torch.float32)\n",
    "\n",
    "            return tags_one_hot\n",
    "\n",
    "\n",
    "    # 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "\n",
    "    labels = np.unique(\"|\".join(list(df.newTag)).split(\"|\"))\n",
    "    print(f\"labels: {labels}\")\n",
    "\n",
    "    trainset = NER_Dataset(\"train\", tokenizer=tokenizer, data_path=data_path, labels= labels)\n",
    "\n",
    "    from torch.utils.data import DataLoader, IterableDataset\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    def create_mini_batch(samples):\n",
    "        tokens_tensors = [s[0] for s in samples]\n",
    "        segments_tensors = [s[1] for s in samples]\n",
    "\n",
    "        # 測試集有 labels\n",
    "        if samples[0][2] is not None:\n",
    "            label_ids = [s[2] for s in samples]\n",
    "            label_ids = pad_sequence(label_ids, \n",
    "                                      batch_first=True)\n",
    "        else:\n",
    "            label_ids = None\n",
    "\n",
    "        # zero pad 到同一序列長度\n",
    "        tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                      batch_first=True)\n",
    "        segments_tensors = pad_sequence(segments_tensors, \n",
    "                                        batch_first=True)\n",
    "\n",
    "        # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "        # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "        masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                    dtype=torch.long)\n",
    "        masks_tensors = masks_tensors.masked_fill(\n",
    "            tokens_tensors != 0, 1)\n",
    "\n",
    "        return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    trainset.id2label = {}\n",
    "    for key in trainset.label_map.keys():\n",
    "        trainset.id2label[trainset.label_map[key]] = key\n",
    "\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                             collate_fn=create_mini_batch)\n",
    "\n",
    "    data = next(iter(trainloader))\n",
    "\n",
    "    tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, label_ids = data\n",
    "\n",
    "    '''print(f\"\"\"\n",
    "    tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "    {tokens_tensors}\n",
    "    ------------------------\n",
    "    segments_tensors.shape = {segments_tensors.shape}\n",
    "    {segments_tensors}\n",
    "    ------------------------\n",
    "    masks_tensors.shape    = {masks_tensors.shape}\n",
    "    {masks_tensors}\n",
    "    ------------------------\n",
    "    label_ids.shape        = {label_ids.shape}\n",
    "    {label_ids}\n",
    "    \"\"\")'''\n",
    "    \n",
    "    trainset.id2label = {}\n",
    "    for key in trainset.label_map.keys():\n",
    "        trainset.id2label[trainset.label_map[key]] = key\n",
    "\n",
    "    \n",
    "    return all_tags, trainset, trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4c9b485",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:02:32.574289Z",
     "start_time": "2021-07-31T23:02:28.343508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: ['B-art' 'B-eve' 'B-geo' 'B-gpe' 'B-nat' 'B-org' 'B-per' 'B-tim'\n",
      " 'CountryCode' 'CryptoCurrencyCode' 'CurrencyCode' 'Event' 'Float' 'I-art'\n",
      " 'I-eve' 'I-geo' 'I-gpe' 'I-nat' 'I-org' 'I-per' 'I-tim' 'Integer'\n",
      " 'Location' 'Month' 'O' 'Object' 'Party' 'Race' 'SpecialTerm'\n",
      " 'TemporalUnit' 'Time' 'Timezone' 'US_States']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8408/2177286180.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"labels: {labels}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNER_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "\n",
    "all_tags = df.newTag\n",
    "\n",
    "all_tags = set(all_tags)\n",
    "\n",
    "all_tags = \"|\".join(all_tags)\n",
    "all_tags = all_tags.split(\"|\")\n",
    "all_tags = set(all_tags)\n",
    "all_tags = list(all_tags)\n",
    "\n",
    "\n",
    "def process_csv(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    tags = df.groupby(\"Sentence #\")[\"newTag\"].apply(list).values\n",
    "    return sentences, tags\n",
    "\n",
    "sentences, tags = process_csv(data_path)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NER_Dataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer, data_path, labels):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.sentences, self.tags = process_csv(data_path)\n",
    "        self.len = len(self.sentences)\n",
    "\n",
    "\n",
    "        if mode != \"test\":\n",
    "            self.label_map = {}\n",
    "            for i in range(len(labels)):\n",
    "                self.label_map[labels[i]] = i\n",
    "\n",
    "            possible_labels = np.array(range(len(labels))).reshape(-1, 1)\n",
    "            self.oneHotEncoder = OneHotEncoder()\n",
    "            self.oneHotEncoder.fit(possible_labels)\n",
    "        else:\n",
    "            self.label_map = None\n",
    "\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        self.O_label = self.label_map[\"O\"]\n",
    "\n",
    "\n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            label = [\"O\"] + self.tags[idx] + [\"O\"]\n",
    "\n",
    "            label = np.array(label)\n",
    "            label = label.reshape(-1,1)\n",
    "\n",
    "            label = np.apply_along_axis(self.split_one_hot_multiTags, 1, label)\n",
    "            label_tensor = torch.tensor(label, dtype = torch.float32)\n",
    "\n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = ['[CLS]']\n",
    "        word_pieces += self.sentences[idx]\n",
    "        word_pieces += ['[SEP]']\n",
    "\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "\n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0\n",
    "        segments_tensor = torch.zeros_like(tokens_tensor)\n",
    "\n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def split_one_hot_multiTags(self, tags):\n",
    "        # tags = ['B-org|Party|String']\n",
    "        tags = tags[0]\n",
    "        tags = tags.split(\"|\")\n",
    "\n",
    "\n",
    "        tags_num = list(map(lambda x: self.label_map[x], tags))\n",
    "        #[5, 20, 23]\n",
    "\n",
    "        tags_num = np.array(tags_num).reshape(-1,1)\n",
    "\n",
    "        tags_one_hot = self.oneHotEncoder.transform(tags_num).toarray()\n",
    "\n",
    "        tags_one_hot = tags_one_hot.sum(axis = 0)\n",
    "\n",
    "        #return torch.tensor(tags_one_hot, dtype = torch.float32)\n",
    "\n",
    "        return tags_one_hot\n",
    "\n",
    "\n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "\n",
    "df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "\n",
    "labels = np.unique(\"|\".join(list(df.newTag)).split(\"|\"))\n",
    "print(f\"labels: {labels}\")\n",
    "\n",
    "trainset = NER_Dataset(\"train\", tokenizer=tokenizer, data_path=data_path, labels= labels)\n",
    "\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "\n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = [s[2] for s in samples]\n",
    "        label_ids = pad_sequence(label_ids, \n",
    "                                  batch_first=True)\n",
    "    else:\n",
    "        label_ids = None\n",
    "\n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "\n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "\n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trainset.id2label = {}\n",
    "for key in trainset.label_map.keys():\n",
    "    trainset.id2label[trainset.label_map[key]] = key\n",
    "\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)\n",
    "\n",
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "'''print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")'''\n",
    "\n",
    "trainset.id2label = {}\n",
    "for key in trainset.label_map.keys():\n",
    "    trainset.id2label[trainset.label_map[key]] = key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf58632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
