{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64f2ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:04.364343Z",
     "start_time": "2021-07-31T23:11:04.031714Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ced4b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:04.409901Z",
     "start_time": "2021-07-31T23:11:04.366860Z"
    }
   },
   "outputs": [],
   "source": [
    "from telegram_notifier import send_message as telegram_bot_sendtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beafcfe7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:04.863601Z",
     "start_time": "2021-07-31T23:11:04.414523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 1.8.1\n",
      "transformers (Adapter) Version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"transformers (Adapter) Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aff5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add515cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:09.166123Z",
     "start_time": "2021-07-31T23:11:04.865561Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215447e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:16.520717Z",
     "start_time": "2021-07-31T23:11:09.168900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: ['B-art' 'B-eve' 'B-geo' 'B-gpe' 'B-nat' 'B-org' 'B-per' 'B-tim'\n",
      " 'CountryCode' 'CryptoCurrencyCode' 'CurrencyCode' 'Event' 'Float' 'I-art'\n",
      " 'I-eve' 'I-geo' 'I-gpe' 'I-nat' 'I-org' 'I-per' 'I-tim' 'Integer'\n",
      " 'Location' 'Month' 'O' 'Object' 'Party' 'Race' 'SpecialTerm'\n",
      " 'TemporalUnit' 'Time' 'Timezone' 'US_States']\n"
     ]
    }
   ],
   "source": [
    "from ner_dataset import get_trainset_data_loader\n",
    "\n",
    "all_tags, trainset, trainloader = get_trainset_data_loader(tokenizer, BATCH_SIZE=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b00c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c29d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:17.290830Z",
     "start_time": "2021-07-31T23:11:16.523615Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModelWithHeads\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(all_tags),\n",
    "    label2id = trainset.label_map, \n",
    "    id2label = trainset.id2label\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af991b5d",
   "metadata": {},
   "source": [
    "name = model.load_adapter(\"./save_adapters/ALL_tag_0730\")\n",
    "model.add_tagging_head(\n",
    "        name,\n",
    "        num_labels=len(trainset.label_map.keys()), overwrite_ok=True\n",
    "      )\n",
    "model.train_adapter(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ef36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac2ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45587fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T07:49:00.550764Z",
     "start_time": "2021-07-30T07:49:00.533260Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d52772",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:17.299994Z",
     "start_time": "2021-07-31T23:11:17.294579Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b4417c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:17.311698Z",
     "start_time": "2021-07-31T23:11:17.301346Z"
    }
   },
   "outputs": [],
   "source": [
    "device_id = 1\n",
    "device = torch.device(f\"cuda:{device_id}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d44620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bd2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6c42b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T23:11:17.317468Z",
     "start_time": "2021-07-31T23:11:17.314173Z"
    }
   },
   "outputs": [],
   "source": [
    "all_tags = ['Float','TemporalUnit','I-gpe','CountryCode','CurrencyCode','Timezone','CryptoCurrencyCode','Month','Party','B-tim','I-art','Time','B-per','B-gpe','B-geo','O','Location','Event','I-nat','Race','B-org','I-geo','I-tim','I-eve','SpecialTerm','B-art','US_States','B-eve','I-org','B-nat','Object','I-per','Integer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bc072",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.724Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Float: epoch 0\n",
      "\tLoss: 0.9188047647476196\n",
      "\tLoss: 0.003375146072357893\n",
      "\tLoss: 0.0009547284571453929\n",
      "\tLoss: 0.005346503108739853\n",
      "\n",
      "Float: epoch 1\n",
      "\tLoss: 0.0033383220434188843\n",
      "\tLoss: 0.0005077467649243772\n",
      "\tLoss: 0.0007603301201015711\n",
      "\tLoss: 0.00025108555564656854\n",
      "\n",
      "Float: epoch 2\n",
      "\tLoss: 0.0009377290843985975\n",
      "\tLoss: 0.00014787307009100914\n",
      "\tLoss: 6.494282570201904e-05\n",
      "\tLoss: 0.0001601462863618508\n",
      "\n",
      "Float: epoch 3\n",
      "\tLoss: 0.0008315640152432024\n",
      "\tLoss: 0.0002520382113289088\n",
      "\tLoss: 4.814233398064971e-05\n",
      "\tLoss: 0.00024878120166249573\n",
      "\n",
      "Skip TemporalUnit.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModelWithHeads were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.encoder.layer.0.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.Float.adapter_up.bias', 'heads.Float.1.weight', 'heads.Float.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I-gpe: epoch 0\n",
      "\tLoss: 0.5326796174049377\n",
      "\tLoss: 0.0003524461935739964\n",
      "\tLoss: 0.0002133602974936366\n",
      "\tLoss: 0.005768406204879284\n",
      "\n",
      "I-gpe: epoch 1\n",
      "\tLoss: 0.00017466171993874013\n",
      "\tLoss: 0.0001283957390114665\n",
      "\tLoss: 0.0001736785634420812\n",
      "\tLoss: 0.0032435511238873005\n",
      "\n",
      "I-gpe: epoch 2\n",
      "\tLoss: 6.654494791291654e-05\n",
      "\tLoss: 0.00016311307263094932\n",
      "\tLoss: 0.00013056714669801295\n",
      "\tLoss: 0.0005585987819358706\n",
      "\n",
      "I-gpe: epoch 3\n",
      "\tLoss: 3.449988071224652e-05\n",
      "\tLoss: 0.0002468148013576865\n",
      "\tLoss: 0.0001158657469204627\n",
      "\tLoss: 0.00019493099534884095\n",
      "\n",
      "Skip CountryCode.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModelWithHeads: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModelWithHeads were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bert.encoder.layer.0.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.0.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.0.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.0.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.0.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.1.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.1.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.1.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.1.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.2.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.2.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.2.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.2.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.3.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.3.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.3.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.3.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.4.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.4.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.4.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.4.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.5.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.5.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.5.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.5.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.6.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.6.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.6.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.6.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.7.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.7.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.7.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.7.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.8.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.8.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.8.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.8.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.9.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.9.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.9.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.9.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.10.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.10.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.10.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.10.output.adapters.I-gpe.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.Float.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.Float.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.Float.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.Float.adapter_up.bias', 'bert.encoder.layer.11.output.adapters.I-gpe.adapter_down.0.weight', 'bert.encoder.layer.11.output.adapters.I-gpe.adapter_down.0.bias', 'bert.encoder.layer.11.output.adapters.I-gpe.adapter_up.weight', 'bert.encoder.layer.11.output.adapters.I-gpe.adapter_up.bias', 'heads.Float.1.weight', 'heads.Float.1.bias', 'heads.I-gpe.1.weight', 'heads.I-gpe.1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CurrencyCode: epoch 0\n",
      "\tLoss: 0.6318280100822449\n",
      "\tLoss: 0.010057496838271618\n",
      "\tLoss: 0.006364795845001936\n",
      "\tLoss: 0.0013016602024435997\n"
     ]
    }
   ],
   "source": [
    "for index, tag in enumerate(all_tags):\n",
    "    if index % 2 == device_id:\n",
    "        print(f\"\\nSkip {tag}.\\n\")\n",
    "        continue\n",
    "    model = BertModelWithHeads.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        )\n",
    "\n",
    "\n",
    "    try:\n",
    "        model.add_adapter(tag)\n",
    "        model.add_tagging_head(\n",
    "            tag,\n",
    "            num_labels=1\n",
    "          )\n",
    "    except: pass\n",
    "    model.train_adapter(tag)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "                    {\n",
    "                        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 1e-5,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                ]\n",
    "    optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters, lr=1e-4)\n",
    "    \n",
    "    for epoch in range(4):\n",
    "        print(f\"\\n{tag}: epoch {epoch}\")\n",
    "        for i, data in enumerate(trainloader):\n",
    "\n",
    "            tokens_tensors, segments_tensors, \\\n",
    "            masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "            outputs = model(input_ids = tokens_tensors,\n",
    "                attention_mask=masks_tensors,\n",
    "                token_type_ids=segments_tensors)\n",
    "\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            current_label = labels.view(-1, labels.shape[-1])[:, trainset.label_map[tag]]\n",
    "            current_label = current_label.view(-1)\n",
    "\n",
    "            active_logits = logits.view(-1, logits.shape[-1])[masks_tensors.view(-1) == 1]\n",
    "            active_labels = current_label[masks_tensors.view(-1)== 1]\n",
    "\n",
    "            actual = current_label[masks_tensors.view(-1)== 1].float().view(-1,1)\n",
    "            \"\"\"\n",
    "\n",
    "            actual = torch.ones(active_logits.shape, device = device)\n",
    "\n",
    "            actual[:, 0] = (active_labels == 0).long()\n",
    "            actual[:, 1] = (active_labels == 1).long()\"\"\"\n",
    "\n",
    "\n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "            loss = loss_fct(active_logits, actual)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"\\tLoss: {loss}\")\n",
    "        telegram_bot_sendtext(f\"\\n{tag}: epoch {epoch}, loss = {loss}\")\n",
    "        filename = f\"{tag}_epoch_{epoch}_0801_bert\"\n",
    "        model.save_adapter(f\"./save_adapters/{filename}\", model.active_adapters[0])\n",
    "        model.save_head(f\"./save_heads/{filename}\", model.active_head)\n",
    "    filename = f\"{tag}_0731\"\n",
    "    model.save_adapter(f\"./save_adapters/{filename}\", model.active_adapters[0])\n",
    "    model.save_head(f\"./save_heads/{filename}\", model.active_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de25ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30771ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e908c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce26589",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.729Z"
    }
   },
   "outputs": [],
   "source": [
    "label_id_mapping = trainset.label_map\n",
    "\n",
    "id_label_mapping = dict()\n",
    "for key in label_id_mapping.keys():\n",
    "    id_label_mapping[label_id_mapping[key]] = key\n",
    "\n",
    "def test_model(model, sentence, device = \"cpu\"):\n",
    "    tokenized_sentence = torch.tensor([tokenizer.encode(sentence)])\n",
    "    pos = torch.tensor([[0] * len(tokenized_sentence)])\n",
    "    tags = torch.tensor([[1] * len(tokenized_sentence)])\n",
    "\n",
    "    model = model.to(device)\n",
    "    outputs = model(input_ids=tokenized_sentence.to(device), \n",
    "                    token_type_ids=pos.to(device), \n",
    "                    attention_mask=tags.to(device))\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    _, pred_labels = torch.max(logits, 2)\n",
    "\n",
    "    out_labels = []\n",
    "    for row in pred_labels:\n",
    "        result = list(map(lambda x: id_label_mapping[int(x)], row))\n",
    "        out_labels.append(result)\n",
    "    #return tokenizer.tokenize(sentence), out_labels[0], logits\n",
    "    return tokenizer.tokenize(sentence), out_labels[0][1:-1], logits[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46deba",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.730Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Dan will be deemed to have completed its delivery obligations before 2021-7-5 if in Niall's opinion, the Jeep Car satisfies the Acceptance Criteria, and Niall notifies Dan in writing that it is accepting the Jeep Car.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a87d35",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.732Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = torch.tensor([tokenizer.encode(sentence)])\n",
    "pos = torch.tensor([[0] * len(tokenized_sentence)])\n",
    "tags = torch.tensor([[1] * len(tokenized_sentence)])\n",
    "\n",
    "model = model.to(device)\n",
    "outputs = model(input_ids=tokenized_sentence.to(device), \n",
    "                token_type_ids=pos.to(device), \n",
    "                attention_mask=tags.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90e517",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.733Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, text in enumerate(tokenizer.tokenize(sentence)):\n",
    "    print(f\"{text}: {outputs[0].view(-1)[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1088c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656abe32",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.736Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Dan Will be deemed to have completed its delivery obligations before 2021-7-5 if in Niall's opinion, the Jeep Car satisfies the Acceptance Criteria, and Niall notifies Dan in writing that it is accepting the Jeep Car.\"\n",
    "sen, pred, logits = test_model(model, sentence, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16220ab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.737Z"
    }
   },
   "outputs": [],
   "source": [
    "a = tokenizer.tokenize(sentence)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1f6fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d937d0b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.740Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae43e1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.741Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246a1d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.742Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "def interact_word(i):\n",
    "    print(i)\n",
    "    print(sen[i])\n",
    "    target = out[i]\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        print(f\"{i} {id_label_mapping[i].ljust(6)} \\t: {target[i]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43105772",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.745Z"
    }
   },
   "outputs": [],
   "source": [
    "out = logits[0]\n",
    "interact(lambda x: interact_word(x), x=widgets.IntSlider(min=0, max=len(sen)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf934d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b2200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea35621",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-30T07:58:41.852Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bdd8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0aa6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e508bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcfc38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439ea919",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-31T23:10:59.755Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e97b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
