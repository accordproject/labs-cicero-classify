{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9a0391",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:17:48.198178Z",
     "start_time": "2021-07-31T22:17:47.389419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 1.8.1\n",
      "transformers (Adapter) Version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "\n",
    "import transformers\n",
    "print(f\"transformers (Adapter) Version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f380fe09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:00.940990Z",
     "start_time": "2021-07-31T22:17:48.200798Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5d9ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:01.487520Z",
     "start_time": "2021-07-31T22:18:00.943743Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"./NER_multilabel_data_v2.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "all_tags = df.newTag\n",
    "\n",
    "all_tags = set(all_tags)\n",
    "\n",
    "all_tags = \"|\".join(all_tags)\n",
    "all_tags = all_tags.split(\"|\")\n",
    "all_tags = set(all_tags)\n",
    "all_tags = list(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2f033b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:03.656411Z",
     "start_time": "2021-07-31T22:18:01.490640Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_csv(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    tags = df.groupby(\"Sentence #\")[\"newTag\"].apply(list).values\n",
    "    return sentences, tags\n",
    "\n",
    "sentences, tags = process_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1860f16a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:07.312256Z",
     "start_time": "2021-07-31T22:18:03.659012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: ['B-art' 'B-eve' 'B-geo' 'B-gpe' 'B-nat' 'B-org' 'B-per' 'B-tim'\n",
      " 'CountryCode' 'CryptoCurrencyCode' 'CurrencyCode' 'Event' 'Float' 'I-art'\n",
      " 'I-eve' 'I-geo' 'I-gpe' 'I-nat' 'I-org' 'I-per' 'I-tim' 'Integer'\n",
      " 'Location' 'Month' 'O' 'Object' 'Party' 'Race' 'SpecialTerm'\n",
      " 'TemporalUnit' 'Time' 'Timezone' 'US_States']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class NER_Dataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer, data_path, labels):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.sentences, self.tags = process_csv(data_path)\n",
    "        self.len = len(self.sentences)\n",
    "        \n",
    "\n",
    "        if mode != \"test\":\n",
    "            self.label_map = {}\n",
    "            for i in range(len(labels)):\n",
    "                self.label_map[labels[i]] = i\n",
    "                \n",
    "            possible_labels = np.array(range(len(labels))).reshape(-1, 1)\n",
    "            self.oneHotEncoder = OneHotEncoder()\n",
    "            self.oneHotEncoder.fit(possible_labels)\n",
    "        else:\n",
    "            self.label_map = None\n",
    "        \n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        self.O_label = self.label_map[\"O\"]\n",
    "\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            label = [\"O\"] + self.tags[idx] + [\"O\"]\n",
    "\n",
    "            label = np.array(label)\n",
    "            label = label.reshape(-1,1)\n",
    "\n",
    "            label = np.apply_along_axis(self.split_one_hot_multiTags, 1, label)\n",
    "            label_tensor = torch.tensor(label, dtype = torch.float32)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = ['[CLS]']\n",
    "        word_pieces += self.sentences[idx]\n",
    "        word_pieces += ['[SEP]']\n",
    "        \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0\n",
    "        segments_tensor = torch.zeros_like(tokens_tensor)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def split_one_hot_multiTags(self, tags):\n",
    "        # tags = ['B-org|Party|String']\n",
    "        tags = tags[0]\n",
    "        tags = tags.split(\"|\")\n",
    "\n",
    "\n",
    "        tags_num = list(map(lambda x: self.label_map[x], tags))\n",
    "        #[5, 20, 23]\n",
    "\n",
    "        tags_num = np.array(tags_num).reshape(-1,1)\n",
    "\n",
    "        tags_one_hot = self.oneHotEncoder.transform(tags_num).toarray()\n",
    "\n",
    "        tags_one_hot = tags_one_hot.sum(axis = 0)\n",
    "\n",
    "        #return torch.tensor(tags_one_hot, dtype = torch.float32)\n",
    "\n",
    "        return tags_one_hot\n",
    "    \n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "\n",
    "df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "\n",
    "labels = np.unique(\"|\".join(list(df.newTag)).split(\"|\"))\n",
    "print(f\"labels: {labels}\")\n",
    "\n",
    "trainset = NER_Dataset(\"train\", tokenizer=tokenizer, data_path=data_path, labels= labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557e8a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.627230Z",
     "start_time": "2021-07-31T22:18:07.314393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainset.id2label = {}\n",
    "for key in trainset.label_map.keys():\n",
    "    trainset.id2label[trainset.label_map[key]] = key\n",
    "\n",
    "from transformers import RobertaConfig, RobertaModelWithHeads\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=len(all_tags),\n",
    "    label2id = trainset.label_map, \n",
    "    id2label = trainset.id2label\n",
    ")\n",
    "model = RobertaModelWithHeads.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e54ecf77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.712477Z",
     "start_time": "2021-07-31T22:18:11.629505Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting existing head 'Float'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./save_heads/Float_0730', 'Float')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_name = \"Float\"\n",
    "name = model.load_adapter(f\"./save_adapters/{adapter_name}_0730\")\n",
    "model.load_head(f\"./save_heads/{adapter_name}_0730\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0c59f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:56:50.281042Z",
     "start_time": "2021-07-30T02:56:50.208807Z"
    }
   },
   "source": [
    "adapter_name = \"All_tag_2\"\n",
    "model.load_adapter(f\"./save_adapters/{adapter_name}\")\n",
    "model.load_head(f\"./save_heads/{adapter_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54fb0cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.717142Z",
     "start_time": "2021-07-31T22:18:11.714065Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.adapters.composition import Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be7e7da2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.721120Z",
     "start_time": "2021-07-31T22:18:11.718783Z"
    }
   },
   "outputs": [],
   "source": [
    "parallel = Parallel(\"ALL2\", \"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0275501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.961311Z",
     "start_time": "2021-07-31T22:18:11.723800Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No adapter with name 'ALL2' found. Please make sure that all specified adapters are correctly loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7575/1361435234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_active_adapters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/data/anaconda3/envs/adapter/lib/python3.7/site-packages/transformers/adapters/heads.py\u001b[0m in \u001b[0;36mset_active_adapters\u001b[0;34m(self, adapter_setup, skip_layers)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0madapter_setup\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0madapters\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mCan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfusion\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstacking\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \"\"\"\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_active_adapters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;31m# use last adapter name as name of prediction head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_adapters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/data/anaconda3/envs/adapter/lib/python3.7/site-packages/transformers/adapters/model_mixin.py\u001b[0m in \u001b[0;36mset_active_adapters\u001b[0;34m(self, adapter_setup, skip_layers)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0madapter_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 183\u001b[0;31m                         \u001b[0;34mf\"No adapter with name '{adapter_name}' found. Please make sure that all specified adapters are correctly loaded.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m                     )\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No adapter with name 'ALL2' found. Please make sure that all specified adapters are correctly loaded."
     ]
    }
   ],
   "source": [
    "model.set_active_adapters(parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db51c11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.962537Z",
     "start_time": "2021-07-31T22:18:11.962525Z"
    }
   },
   "outputs": [],
   "source": [
    "model.active_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf3993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a935fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434df5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902130e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.963612Z",
     "start_time": "2021-07-31T22:18:11.963600Z"
    }
   },
   "outputs": [],
   "source": [
    "label_id_mapping = trainset.label_map\n",
    "\n",
    "id_label_mapping = dict()\n",
    "for key in label_id_mapping.keys():\n",
    "    id_label_mapping[label_id_mapping[key]] = key\n",
    "\n",
    "def test_model(model, sentence, device = \"cpu\"):\n",
    "    tokenized_sentence = torch.tensor([tokenizer.encode(sentence)])\n",
    "    pos = torch.tensor([[0] * len(tokenized_sentence)])\n",
    "    tags = torch.tensor([[1] * len(tokenized_sentence)])\n",
    "\n",
    "    model = model.to(device)\n",
    "    outputs = model(input_ids=tokenized_sentence.to(device), \n",
    "                    token_type_ids=pos.to(device), \n",
    "                    attention_mask=tags.to(device))\n",
    "\n",
    "    logits = outputs[1][0]\n",
    "\n",
    "    _, pred_labels = torch.max(logits, 2)\n",
    "\n",
    "    out_labels = []\n",
    "    for row in pred_labels:\n",
    "        result = list(map(lambda x: id_label_mapping[int(x)], row))\n",
    "        out_labels.append(result)\n",
    "    #return tokenizer.tokenize(sentence), out_labels[0], logits\n",
    "    return tokenizer.tokenize(sentence), out_labels[0][1:-1], logits[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f243f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.964307Z",
     "start_time": "2021-07-31T22:18:11.964296Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Dan will be deemed to have completed its delivery for 8.2 obligations before 2021-7-5 if in Niall's opinion, the Jeep Car satisfies the Acceptance Criteria, and Niall notifies Dan in writing that it is accepting the Jeep Car.\"\n",
    "sen, pred, logits = test_model(model, sentence, device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef2767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.965100Z",
     "start_time": "2021-07-31T22:18:11.965089Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c8084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.965955Z",
     "start_time": "2021-07-31T22:18:11.965944Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543a1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-30T02:28:27.363348Z",
     "start_time": "2021-07-30T02:28:27.353581Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd7d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.966751Z",
     "start_time": "2021-07-31T22:18:11.966740Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "def interact_word(i):\n",
    "    print(i)\n",
    "    print(f\"{sen[i]}: {pred[i]}\")\n",
    "    target = out[i]\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        print(f\"{i} {id_label_mapping[i].ljust(6)} \\t: {target[i]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4ed4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T22:18:11.967519Z",
     "start_time": "2021-07-31T22:18:11.967508Z"
    }
   },
   "outputs": [],
   "source": [
    "out = logits[0]\n",
    "interact(lambda x: interact_word(x), x=widgets.IntSlider(min=0, max=len(sen)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93ce16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f5975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
