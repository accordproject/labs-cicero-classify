{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe_W__U6uFBs"
   },
   "source": [
    "First we need to download the dataset. In this case we use a datasets containing poems. By doing so we train the model to create its own poems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBuzA0h3n4-z",
    "outputId": "aded08bc-6647-43d9-c2e3-0bdd831e3a33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset poem_sentiment (/home/eason/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 105\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"poem_sentiment\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neZt29NKuT11"
   },
   "source": [
    "Before training we need to preprocess the dataset. We tokenize the entries in the dataset and remove all columns we don't need to train the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3Lz6h9Zo9t0",
    "outputId": "9fdb9f57-6aaf-4d09-aa78-07e4049029f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a71e40510d44b387ac5a3b677b9e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7fa3f0b39942739fb5edd0cb456ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc818187c6d44ee900e604e19e9a3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  encoding = tokenizer(batch[\"verse_text\"])\n",
    "  # For language modeling the labels need to be the input_ids\n",
    "  #encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "  return encoding\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# The GPT-2 tokenizer does not have a padding token. In order to process the data \n",
    "# in batches we set one here \n",
    "column_names = dataset[\"train\"].column_names\n",
    "dataset = dataset.map(encode_batch, remove_columns=column_names, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqJWsH5_ENkb"
   },
   "source": [
    "Next we concatenate the documents in the dataset and create chunks with a length of `block_size`. This is beneficial for language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ff5lRmnA6R7",
    "outputId": "afc8b3c0-d70d-4360-f24f-6bffba720957"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c00b9e4bb6e45e4b3d1ebb2eb5d67e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc02afc674e4d389c2e90bb1b83f5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e24ca5d736404b801a406cf47700fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_size = 50\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "  # Concatenate all texts.\n",
    "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "  # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "  # customize this part to your needs.\n",
    "  total_length = (total_length // block_size) * block_size\n",
    "  # Split by chunks of max_len.\n",
    "  result = {\n",
    "    k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "  }\n",
    "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "  return result\n",
    "\n",
    "dataset = dataset.map(group_texts,batched=True,)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AT56RfOGFXvt"
   },
   "source": [
    "Next we create the model and add our new adapter.Let's just call it `poem` since it is trained to create new poems. Then we activate it and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ioLpFbOfnPE6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# add new adapter\n",
    "model.add_adapter(\"poem\")\n",
    "# activate adapter for training\n",
    "model.train_adapter(\"poem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkCUz8B6Fw5E"
   },
   "source": [
    "The last thing we need to do before we can start training is create the trainer. As trainingsargumnÃ©nts we choose a learningrate of 1e-4. Feel free to play around with the paraeters and see how they affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "stDwnIEApmNu"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./examples\", \n",
    "  do_train=True,\n",
    "  remove_unused_columns=False,\n",
    "  learning_rate=5e-4,\n",
    "  num_train_epochs=3,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"], \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "id": "Mtjczmcxqv-l",
    "outputId": "1c386f87-f5cd-4791-d4de-4fd8efe74583"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eason/data/anaconda3/envs/adapter/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=54, training_loss=4.2605831004955155, metrics={'train_runtime': 10.0689, 'train_samples_per_second': 5.363, 'total_flos': 26502744153600.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pkNcsRzGCQo"
   },
   "source": [
    "Now that we have a trained udapter we save it for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"what a \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6iiEOwfsdVd",
    "outputId": "eac599d2-2ddf-4d49-e550-6ad7ad14c411"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(PREFIX, return_tensors=\"pt\")\n",
    "encoding = encoding.to(model.device)\n",
    "output_sequence = model.generate(\n",
    "  input_ids=encoding[\"input_ids\"][:,:-1],\n",
    "  attention_mask=encoding[\"attention_mask\"][:,:-1],\n",
    "  do_sample=True,\n",
    "  num_return_sequences=5,\n",
    "  max_length = 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ4uCZaZG68k"
   },
   "source": [
    "Lastly we want to see what the model actually created. Too de this we need to decode the tokens from ids back to words and remove the end of sentence tokens. You can easily use this code with an other dataset. Don't forget to share your adapters at [AdapterHub](https://adapterhub.ml/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TX3QpuWPtrol",
    "outputId": "e5260e2e-7681-49a7-d05e-6f4c4f7aeeef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GENERATED SEQUENCE 1 ===\n",
      "[CLS] what a little wonder does [SEP] [CLS] within that worst [SEP] [CLS] that is a truth. [SEP] [CLS] truth was in trup. and he bed a spread : [SEP] [CLS] and fa\n",
      "=== GENERATED SEQUENCE 2 ===\n",
      "[CLS] what a great mother, [SEP] [CLS] and i have your broos or not that! this, whose stranged, [SEP] [CLS] and that even bleed to [SEP] [CLS] in their time to do the poo\n",
      "=== GENERATED SEQUENCE 3 ===\n",
      "[CLS] what a snow father below : in folin ï¼ like noone ï¼ he likeed means on which a snow ; [SEP] [CLS] their eyes of your growt ï¼ and likely grown \n",
      "=== GENERATED SEQUENCE 4 ===\n",
      "[CLS] what a situations to me, [SEP] [CLS] with me, [SEP] [CLS] what a worrys is, and they't [SEP] [CLS] that there'' their name and i mugul! when, the eagle th\n",
      "=== GENERATED SEQUENCE 5 ===\n",
      "[CLS] what a sack when quink [SEP] [CLS] the mercies seed [UNK] while in the moon, [SEP] [CLS] and my father i will looking be a seats and the dar, [SEP] [CLS] a sea'\n"
     ]
    }
   ],
   "source": [
    " for generated_sequence_idx, generated_sequence in enumerate(output_sequence):\n",
    "        print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "        generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        # Remove EndOfSentence Tokens\n",
    "        text = text[: text.find(tokenizer.pad_token)]\n",
    "\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "05_Text_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "adapter",
   "language": "python",
   "name": "adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
