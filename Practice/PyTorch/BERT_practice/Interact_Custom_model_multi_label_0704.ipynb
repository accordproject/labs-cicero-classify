{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:53:58.689735Z",
     "start_time": "2021-07-04T07:53:54.375728Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    "    BertTokenizer,)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:54:03.685503Z",
     "start_time": "2021-07-04T07:53:58.691563Z"
    }
   },
   "outputs": [],
   "source": [
    "# python\n",
    "from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:54:04.090035Z",
     "start_time": "2021-07-04T07:54:03.687376Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "data_path = \"../NER_test/multiple_tag_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:54:04.095314Z",
     "start_time": "2021-07-04T07:54:04.091453Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_csv(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    tags = df.groupby(\"Sentence #\")[\"multiTag\"].apply(list).values\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:54:10.501746Z",
     "start_time": "2021-07-04T07:54:04.097092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: ['B-art' 'B-eve' 'B-geo' 'B-gpe' 'B-nat' 'B-org' 'B-per' 'B-tim' 'Event'\n",
      " 'I-art' 'I-eve' 'I-geo' 'I-gpe' 'I-nat' 'I-org' 'I-per' 'I-tim'\n",
      " 'Location' 'O' 'Object' 'Party' 'Race' 'SpecialTerm' 'String'\n",
      " 'TemporalUnit']\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class NER_Dataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer, data_path, labels):\n",
    "        assert mode in [\"train\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.sentences, self.tags = process_csv(data_path)\n",
    "        self.len = len(self.sentences)\n",
    "        \n",
    "\n",
    "        if mode != \"test\":\n",
    "            self.label_map = {}\n",
    "            for i in range(len(labels)):\n",
    "                self.label_map[labels[i]] = i\n",
    "                \n",
    "            possible_labels = np.array(range(len(labels))).reshape(-1, 1)\n",
    "            self.oneHotEncoder = OneHotEncoder()\n",
    "            self.oneHotEncoder.fit(possible_labels)\n",
    "        else:\n",
    "            self.label_map = None\n",
    "        \n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        self.O_label = self.label_map[\"O\"]\n",
    "\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            label = [\"O\"] + self.tags[idx] + [\"O\"]\n",
    "\n",
    "            label = np.array(label)\n",
    "            label = label.reshape(-1,1)\n",
    "\n",
    "            label = np.apply_along_axis(self.split_one_hot_multiTags, 1, label)\n",
    "            label_tensor = torch.tensor(label, dtype = torch.float32)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = ['[CLS]']\n",
    "        word_pieces += self.sentences[idx]\n",
    "        word_pieces += ['[SEP]']\n",
    "        \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0\n",
    "        segments_tensor = torch.zeros_like(tokens_tensor)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def split_one_hot_multiTags(self, tags):\n",
    "        # tags = ['B-org|Party|String']\n",
    "        tags = tags[0]\n",
    "        tags = tags.split(\"|\")\n",
    "\n",
    "\n",
    "        tags_num = list(map(lambda x: self.label_map[x], tags))\n",
    "        #[5, 20, 23]\n",
    "\n",
    "        tags_num = np.array(tags_num).reshape(-1,1)\n",
    "\n",
    "        tags_one_hot = self.oneHotEncoder.transform(tags_num).toarray()\n",
    "\n",
    "        tags_one_hot = tags_one_hot.sum(axis = 0)\n",
    "\n",
    "        #return torch.tensor(tags_one_hot, dtype = torch.float32)\n",
    "\n",
    "        return tags_one_hot\n",
    "    \n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "\n",
    "df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "\n",
    "labels = np.unique(\"|\".join(list(df.multiTag)).split(\"|\"))\n",
    "print(f\"labels: {labels}\")\n",
    "\n",
    "trainset = NER_Dataset(\"train\", tokenizer=tokenizer, data_path=data_path, labels= labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:54:10.513430Z",
     "start_time": "2021-07-04T07:54:10.503070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art': 0,\n",
       " 'B-eve': 1,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-nat': 4,\n",
       " 'B-org': 5,\n",
       " 'B-per': 6,\n",
       " 'B-tim': 7,\n",
       " 'Event': 8,\n",
       " 'I-art': 9,\n",
       " 'I-eve': 10,\n",
       " 'I-geo': 11,\n",
       " 'I-gpe': 12,\n",
       " 'I-nat': 13,\n",
       " 'I-org': 14,\n",
       " 'I-per': 15,\n",
       " 'I-tim': 16,\n",
       " 'Location': 17,\n",
       " 'O': 18,\n",
       " 'Object': 19,\n",
       " 'Party': 20,\n",
       " 'Race': 21,\n",
       " 'SpecialTerm': 22,\n",
       " 'String': 23,\n",
       " 'TemporalUnit': 24}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:55:58.907778Z",
     "start_time": "2021-07-04T07:55:58.893867Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "\n",
    "class BertForTokenMultiLabelClassification(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else True#self.config.use_return_dict\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            #inputs_embeds=inputs_embeds,\n",
    "            #output_attentions=output_attentions,\n",
    "            #output_hidden_states=output_hidden_states,\n",
    "            #return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = torch.sigmoid(logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.BCELoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                #active_logits = logits.view(-1, self.num_labels)\n",
    "                #active_labels = torch.where(\n",
    "                #    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                #)\n",
    "\n",
    "                active_logits = logits.view(-1, self.num_labels)[attention_mask.view(-1)== 1]\n",
    "                active_labels = labels.view(-1, self.num_labels)[attention_mask.view(-1)== 1]\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:56:03.006620Z",
     "start_time": "2021-07-04T07:55:59.846958Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenMultiLabelClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenMultiLabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenMultiLabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenMultiLabelClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "NUM_LABELS = len(labels)\n",
    "model = BertForTokenMultiLabelClassification.from_pretrained(model_name, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:56:03.015660Z",
     "start_time": "2021-07-04T07:56:03.008446Z"
    }
   },
   "outputs": [],
   "source": [
    "label_id_mapping = trainset.label_map\n",
    "\n",
    "id_label_mapping = dict()\n",
    "for key in label_id_mapping.keys():\n",
    "    id_label_mapping[label_id_mapping[key]] = key\n",
    "\n",
    "def test_model(model, sentence, device = \"cpu\"):\n",
    "    tokenized_sentence = torch.tensor([tokenizer.encode(sentence)])\n",
    "    pos = torch.tensor([[0] * len(tokenized_sentence)])\n",
    "    tags = torch.tensor([[1] * len(tokenized_sentence)])\n",
    "\n",
    "    model = model.to(device)\n",
    "    outputs = model(input_ids=tokenized_sentence.to(device), \n",
    "                    token_type_ids=pos.to(device), \n",
    "                    attention_mask=tags.to(device))\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    _, pred_labels = torch.max(logits, 2)\n",
    "\n",
    "    out_labels = []\n",
    "    for row in pred_labels:\n",
    "        result = list(map(lambda x: id_label_mapping[int(x)], row))\n",
    "        out_labels.append(result)\n",
    "    #return tokenizer.tokenize(sentence), out_labels[0], logits\n",
    "    return tokenizer.tokenize(sentence), out_labels[0][1:-1], logits[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:56:06.963250Z",
     "start_time": "2021-07-04T07:56:03.870673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenMultiLabelClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForTokenMultiLabelClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenMultiLabelClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenMultiLabelClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = BertForTokenMultiLabelClassification.from_pretrained(model_name, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T07:56:11.217314Z",
     "start_time": "2021-07-04T07:56:06.964831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(torch.load(\"./test_models/0704_multi_label_16_epoch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:07.862368Z",
     "start_time": "2021-07-04T06:50:07.759229Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Dan will be deemed to have completed its delivery obligations if in Niall's opinion, the Jeep Car satisfies the Acceptance Criteria, and Niall notifies Dan in writing that it is accepting the Jeep Car.\"\n",
    "sen, pred, logits = test_model(model2, sentence, device = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:13.294331Z",
     "start_time": "2021-07-04T06:50:13.289262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dan', 'will', 'be', 'deemed', 'to', 'have', 'completed', 'its',\n",
       "       'delivery', 'obligations', 'if', 'in', 'Niall', \"'\", 's',\n",
       "       'opinion', ',', 'the', 'Jeep', 'Car', 'sat', '##is', '##fies',\n",
       "       'the', 'A', '##cc', '##ept', '##ance', 'C', '##rite', '##ria', ',',\n",
       "       'and', 'Niall', 'not', '##ifies', 'Dan', 'in', 'writing', 'that',\n",
       "       'it', 'is', 'accepting', 'the', 'Jeep', 'Car', '.'], dtype='<U11')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:18.703481Z",
     "start_time": "2021-07-04T06:50:18.689595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Party', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O',\n",
       "       'Party', 'O', 'O', 'O', 'O', 'O', 'String', 'String', 'O', 'O',\n",
       "       'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Party',\n",
       "       'O', 'O', 'Party', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'String',\n",
       "       'O', 'O'], dtype='<U6')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:22.105134Z",
     "start_time": "2021-07-04T06:50:22.100262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 47, 25])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:26.295731Z",
     "start_time": "2021-07-04T06:50:26.277745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan: Party\n",
      "will: O\n",
      "be: O\n",
      "deemed: O\n",
      "to: O\n",
      "have: O\n",
      "completed: O\n",
      "its: O\n",
      "delivery: O\n",
      "obligations: O\n",
      "if: O\n",
      "in: O\n",
      "Niall: Party\n",
      "': O\n",
      "s: O\n",
      "opinion: O\n",
      ",: O\n",
      "the: O\n",
      "Jeep: String\n",
      "Car: String\n",
      "sat: O\n",
      "##is: O\n",
      "##fies: O\n",
      "the: O\n",
      "A: O\n",
      "##cc: O\n",
      "##ept: O\n",
      "##ance: O\n",
      "C: O\n",
      "##rite: O\n",
      "##ria: O\n",
      ",: O\n",
      "and: O\n",
      "Niall: Party\n",
      "not: O\n",
      "##ifies: O\n",
      "Dan: Party\n",
      "in: O\n",
      "writing: O\n",
      "that: O\n",
      "it: O\n",
      "is: O\n",
      "accepting: O\n",
      "the: O\n",
      "Jeep: String\n",
      "Car: O\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sen)):\n",
    "    print(f\"{sen[i]}: {pred[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:53.087427Z",
     "start_time": "2021-07-04T06:50:53.082332Z"
    }
   },
   "outputs": [],
   "source": [
    "out = logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:53.306674Z",
     "start_time": "2021-07-04T06:50:53.298380Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:53.487532Z",
     "start_time": "2021-07-04T06:50:53.476097Z"
    }
   },
   "outputs": [],
   "source": [
    "def interact_word(i):\n",
    "    print(i)\n",
    "    print(sen[i])\n",
    "    target = out[i]\n",
    "\n",
    "    for i in range(len(target)):\n",
    "        print(f\"{i} {id_label_mapping[i].ljust(6)} \\t: {target[i]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:54.142450Z",
     "start_time": "2021-07-04T06:50:54.138031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Niall'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:54.300472Z",
     "start_time": "2021-07-04T06:50:54.237937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acff6cb089b4773a920e68a45d2e1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', max=46), Output()), _dom_classes=('widget-interact',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(lambda x: interact_word(x), x=widgets.IntSlider(min=0, max=len(sen)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T06:50:54.691799Z",
     "start_time": "2021-07-04T06:50:54.663328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deemed\n",
      "0 B-art  \t: 0.00001\n",
      "1 B-eve  \t: 0.00001\n",
      "2 B-geo  \t: 0.00003\n",
      "3 B-gpe  \t: 0.00003\n",
      "4 B-nat  \t: 0.00001\n",
      "5 B-org  \t: 0.00003\n",
      "6 B-per  \t: 0.00002\n",
      "7 B-tim  \t: 0.00002\n",
      "8 Event  \t: 0.00002\n",
      "9 I-art  \t: 0.00001\n",
      "10 I-eve  \t: 0.00001\n",
      "11 I-geo  \t: 0.00002\n",
      "12 I-gpe  \t: 0.00001\n",
      "13 I-nat  \t: 0.00001\n",
      "14 I-org  \t: 0.00002\n",
      "15 I-per  \t: 0.00002\n",
      "16 I-tim  \t: 0.00001\n",
      "17 Location \t: 0.00003\n",
      "18 O      \t: 0.99992\n",
      "19 Object \t: 0.00002\n",
      "20 Party  \t: 0.00004\n",
      "21 Race   \t: 0.00004\n",
      "22 SpecialTerm \t: 0.00001\n",
      "23 String \t: 0.00006\n",
      "24 TemporalUnit \t: 0.00003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 3\n",
    "print(sen[i])\n",
    "target = out[i]\n",
    "\n",
    "for i in range(len(target)):\n",
    "    print(f\"{i} {id_label_mapping[i].ljust(6)} \\t: {target[i]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-04T01:16:14.258Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(sen[i-1])\n",
    "target = out[i]\n",
    "\n",
    "for i in range(len(target)):\n",
    "    print(f\"{i} {id_label_mapping[i].ljust(6)} \\t: {target[i]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-04T01:16:14.259Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 6\n",
    "print(sen[i-1])\n",
    "target = out[i]\n",
    "\n",
    "for i in range(len(target)):\n",
    "    print(f\"{i} {id_label_mapping[i].ljust(6)} \\t: {target[i]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
